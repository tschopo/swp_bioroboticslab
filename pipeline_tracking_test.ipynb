{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:11.188351Z",
     "start_time": "2019-03-19T10:45:11.181191Z"
    }
   },
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=theano\n",
    "%env THEANO_FLAGS=floatX=float32,device=cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:17.562983Z",
     "start_time": "2019-03-19T10:45:11.655135Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport bb_behavior\n",
    "%aimport bb_behavior.plot\n",
    "%aimport bb_behavior.tracking\n",
    "%aimport bb_behavior.tracking.pipeline\n",
    "\n",
    "import bb_behavior\n",
    "import bb_behavior.plot\n",
    "import bb_behavior.tracking\n",
    "import bb_behavior.tracking.pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:17.582372Z",
     "start_time": "2019-03-19T10:45:17.566595Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook # progress bar\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from bb_tracking.data.constants import DETKEY\n",
    "#from bb_tracking.tracking import score_id_sim_v\n",
    "from bb_tracking.tracking import distance_orientations_v, distance_positions_v\n",
    "\n",
    "from bb_behavior.tracking.pipeline import detect_markers_in_video\n",
    "from bb_behavior.tracking.pipeline import track_detections_dataframe\n",
    "from bb_behavior.tracking.pipeline import display_tracking_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:07:29.276297Z",
     "start_time": "2019-03-15T11:07:09.400050Z"
    }
   },
   "outputs": [],
   "source": [
    "from bb_behavior.tracking.pipeline import get_default_pipeline\n",
    "default_pipeline = None\n",
    "default_pipeline = get_default_pipeline(localizer_threshold=\"0.50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:17.595118Z",
     "start_time": "2019-03-19T10:45:17.585372Z"
    },
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "# Hilfsfunktionen\n",
    "def filename_to_datestring(filname):\n",
    "    \"\"\"\n",
    "    filename can be path\n",
    "    \"\"\"\n",
    "    return os.path.split(filname)[-1].split('.')[0].split('_')[1]\n",
    "\n",
    "def datestring_to_filename(datestring, prefix=\"e00_\"):\n",
    "    return config[\"videos_dir\"] + prefix + datestring + \".h264\"\n",
    "\n",
    "def string_to_timestamp(datestring):\n",
    "    \"\"\" \n",
    "    params\n",
    "        string: format 2018-08-19-01-08-13\n",
    "    output\n",
    "        unix timestamp (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    return time.mktime(time.strptime(datestring, \"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "def timestamp_to_string(timestamp):\n",
    "    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:19.545269Z",
     "start_time": "2019-03-19T10:45:19.534453Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get all videos between timestamp_in and timestamp_out\n",
    "def get_videos_between(timestamp_in, timestamp_out, all_paths):\n",
    "    \"\"\" returns all video between timestamp_in and timestamp out (inclusive) \"\"\"\n",
    "    \n",
    "    mask = (all_paths['video'] >= timestamp_in) & (all_paths['video'] <= timestamp_out)\n",
    "    return list(all_paths[mask]['video'])\n",
    "\n",
    "def loadGTD(path):\n",
    "    # read in the test data csv\n",
    "    test_data = pd.read_csv(path)\n",
    "\n",
    "    # convert the full filenames to string timestamps, and sort by timestamp_in\n",
    "    test_data['video'] = test_data['video'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data['video_start_time'] = test_data['timestamp_in'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data['video_end_time'] = test_data['timestamp_out'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data.sort_values(['video_start_time'], inplace=True)\n",
    "\n",
    "    test_data.drop('timestamp_in', 1, inplace=True)\n",
    "    test_data.drop('timestamp_out', 1, inplace=True)\n",
    "    test_data.drop('video', 1, inplace=True)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:20.715778Z",
     "start_time": "2019-03-19T10:45:20.708727Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "config = dict(tag_pixel_diameter=50,\n",
    "              n_frames=None,\n",
    "              confidence_filter_detections=0.08,\n",
    "              confidence_filter_tracks=0.20,\n",
    "              coordinate_scale=1.0,\n",
    "              start_time=None,\n",
    "              fps=10.0,\n",
    "              cam_id=0,\n",
    "              left_leaving_area = 0.3, # Prozente vom Bildschirmrand, zB. bei 1000px und 0.15 -> 0-150px\n",
    "              right_leaving_area = 0.3,\n",
    "              px_x_resolution_vid = 1944,\n",
    "              px_y_resolution_vid = 388,\n",
    "              videos_dir = \"../videos/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:33:28.429823Z",
     "start_time": "2019-03-14T13:33:28.414495Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def detect_tracks(paths, save_to_csv = False):\n",
    "    # Hier passiert das eigentliche Tracken und speichern der Ergebnisse:\n",
    "    num_processed_videos = 0\n",
    "    video_data = dict()\n",
    "    frame_info = None\n",
    "    detections = None\n",
    "    bad_paths = []\n",
    "\n",
    "    for path in tqdm_notebook(paths):\n",
    "        start_time = config[\"start_time\"]\n",
    "        cam_id = config[\"cam_id\"]\n",
    "        try:\n",
    "            num_processed_videos += 1\n",
    "\n",
    "            frame_info, detections = detect_markers_in_video(path,\n",
    "                                                          decoder_pipeline=default_pipeline,#pipeline=pipelines(),\n",
    "                                                         tag_pixel_diameter=config[\"tag_pixel_diameter\"],\n",
    "                                                          n_frames=config[\"n_frames\"],\n",
    "                                                          fps=config[\"fps\"],\n",
    "                                                         progress=\"tqdm_notebook\"\n",
    "                                                )\n",
    "            # Sonst würden keine Tracks erkannt werden -> Fehlermeldung\n",
    "            if len(detections[detections['confidence']>=config[\"confidence_filter_detections\"]]) == 0:\n",
    "                continue\n",
    "            tracks = track_detections_dataframe(detections,\n",
    "                                                tracker=\"tracker.det_score_fun.frag_score_fun.dill\",\n",
    "                                                confidence_filter_detections=config[\"confidence_filter_detections\"],\n",
    "                                               confidence_filter_tracks=config[\"confidence_filter_tracks\"],\n",
    "                                                coordinate_scale=config[\"coordinate_scale\"],\n",
    "                                               )\n",
    "            date_string = filename_to_datestring(path)\n",
    "            tracks['video'] = date_string\n",
    "            video_data[path] = (frame_info, detections, tracks)\n",
    "        except ValueError as err: #tritt auf, wenn Video leer ist. In diesem Fall: überspringe video\n",
    "            try:\n",
    "                bad_paths.append(path)\n",
    "                # wir arbeiten später nochmal mit paths, daher müssen das leere löschen, weil sonst\n",
    "                # in video_data kein zugehöriger Value zu Key = file zu finden ist.\n",
    "            except KeyError as err:\n",
    "                continue\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            raise\n",
    "        # only first vid: break\n",
    "\n",
    "    for bad_path in bad_paths:\n",
    "        paths.remove(bad_path)\n",
    "\n",
    "    #video_data[\"file\"][0] --> frame-info, [1] --> detections, [2] --> tracks\n",
    "    for path in paths:\n",
    "        display_tracking_results(path, video_data[path][0], video_data[path][1], video_data[path][2])\n",
    "\n",
    "    tracks = [video_data[paths[x]][2] for x in range(len(paths))]\n",
    "    tracks = pd.concat(tracks,ignore_index=True)\n",
    "    tracks = tracks.drop(columns=[\"localizerSaliency\", \"beeID\", \"camID\", \"frameIdx\"])\n",
    "\n",
    "    if save_to_csv:\n",
    "        with open(\"tracks.csv\", \"w\") as f:\n",
    "            tracks.to_csv(f)\n",
    "\n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a: Create video path list from all videos in videos_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:33:39.751272Z",
     "start_time": "2019-03-14T13:33:39.745103Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Einen Iterable speichern, der alle Videos in einem Iterable zur Verfügung stellt\n",
    "# Diesen Iterable können wir dann in der nächsten Zelle mit tqdm schön durchlaufen\n",
    "base_directory = config[\"videos_dir\"]\n",
    "paths = [i for i in os.listdir(base_directory) if i.endswith(\".h264\")]\n",
    "for i in range(len(paths)):\n",
    "    paths[i] = base_directory + paths[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b: Create video path list from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:34:54.196867Z",
     "start_time": "2019-03-15T10:34:54.066665Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# read in the test data csv\n",
    "test_data = loadGTD(\"bees_test.csv\")\n",
    "\n",
    "all_videos = []\n",
    "\n",
    "all_paths = pd.DataFrame(glob.glob(os.path.join(config[\"videos_dir\"], '*.h264')), columns=['video'])\n",
    "all_paths['video'] = all_paths['video'].apply(lambda x: filename_to_datestring(x))\n",
    "all_paths.sort_values(['video'])\n",
    "\n",
    "# go through test_data and get all videos between timestamp_in and timestamp_out\n",
    "for index, row in test_data.iterrows():\n",
    "    all_videos += get_videos_between(row['video_start_time'],row['video_end_time'], all_paths)\n",
    "    \n",
    "all_videos = sorted(list(set(all_videos)))\n",
    "paths = [datestring_to_filename(x) for x in all_videos]\n",
    "del all_paths\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:35:09.468983Z",
     "start_time": "2019-03-15T10:35:09.453016Z"
    }
   },
   "outputs": [],
   "source": [
    "tracks = detect_tracks(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwischenschritt: Merge all close Tracks of one bee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:27.276718Z",
     "start_time": "2019-03-19T10:45:27.236611Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS IF TRACKS WHERE ALREADY CALCULATED AND SAVED TO AN .csv\n",
    "# CSV EINLESEN UND SETZEN\n",
    "tracks = pd.read_csv(\"all_tracks.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:28.968489Z",
     "start_time": "2019-03-19T10:45:28.944858Z"
    },
    "code_folding": [
     0,
     39
    ]
   },
   "outputs": [],
   "source": [
    "def gather_tracks(tracks):\n",
    "    \"\"\"\n",
    "    transform the tracks df to : bee_id, [list of positions (x,y)], [list of timestamps], timestamp_of video\n",
    "    \"\"\"\n",
    "\n",
    "    #  transform tracks df to : bee_id, [list of positions (x,y)], [list of timestamps], timestamp_of video\n",
    "    grouped = tracks.groupby(['bee_id','track_id','video'],  as_index=False)['xpos','ypos','timestamp','zrotation']\n",
    "\n",
    "    tracks_ml = grouped.aggregate(lambda x: list(x))\n",
    "\n",
    "    # we don't need track_id anymore\n",
    "    tracks_ml = tracks_ml.drop('track_id', 1)\n",
    "\n",
    "    # add a column: convert video name to timestamp\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video'].apply(lambda x: string_to_timestamp(x))\n",
    "    \n",
    "    # we don't need video anymore\n",
    "    tracks_ml = tracks_ml.drop('video', 1)\n",
    "\n",
    "    # because aggregated: now multiple timestamps per row: --> rename\n",
    "    tracks_ml = tracks_ml.rename(columns={'timestamp': 'timestamps'})\n",
    "\n",
    "    # calculate start time of track by adding timestamp of track (seconds since start of video)\n",
    "    # to timestamp of video (date)\n",
    "    tracks_ml['track_start_time'] = tracks_ml['video_start_time'] + tracks_ml['timestamps'].apply(lambda x: x[0])\n",
    "    tracks_ml['track_end_time'] = tracks_ml['video_start_time'] + tracks_ml['timestamps'].apply(lambda x: x[-1])\n",
    "    \n",
    "    # convert back to string\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: timestamp_to_string(x))\n",
    "    \n",
    "    all_paths = pd.DataFrame(glob.glob(os.path.join(config[\"videos_dir\"], '*.h264')), columns=['video'])\n",
    "    all_paths['video'] = all_paths['video'].apply(lambda x: filename_to_datestring(x))\n",
    "    all_paths.sort_values(['video'])\n",
    "    # get end video\n",
    "    tracks_ml['video_end_time'] =  tracks_ml['track_end_time'].apply(lambda x: timestamp_to_string(x))\n",
    "    tracks_ml['video_end_time'] =  tracks_ml[['video_start_time','video_end_time']].apply(lambda x: get_videos_between(x[0],x[1],all_paths)[-1], axis=1)\n",
    "\n",
    "    return tracks_ml\n",
    "\n",
    "def merge_tracks(tracks_ml, verbose = False):\n",
    "    \"\"\"\n",
    "    in: \n",
    "        tracks_ml: output from gather_tracks(tracks)\n",
    "    out:\n",
    "        same df as in, with merged rows\n",
    "        [bee_id:Float, xpos:[Float], ypos:[Float], zrotation:[Float], timestamps:[Float], video_start_time: String,\n",
    "        video_end_time: String, track_start_time:Float, track_end_time:Float]\n",
    "        \n",
    "    merge tracks of same bee where start and end timestamps are close together\n",
    "    assume there can not be overlapping tracks\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 1. sort: bee_id, start_time\n",
    "    tracks_ml = tracks_ml.sort_values(['bee_id', 'track_start_time'])\n",
    "\n",
    "    \n",
    "    # first convert to timestamp\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: string_to_timestamp(x))\n",
    "    \n",
    "    index = 0\n",
    "    while(True):\n",
    "        row = tracks_ml.iloc[index]\n",
    "        next_row = tracks_ml.iloc[index+1]\n",
    "\n",
    "        # if the tracks are from same bee and the start time of next row is closer then 10s -> merge the rows\n",
    "        # merge rows means, next_row is deleted\n",
    "        if (row['bee_id'] == next_row['bee_id']) and ((next_row['track_start_time'] - row['track_end_time']) < 10):\n",
    "            \n",
    "            if verbose:\n",
    "                print(timestamp_to_string(row['video_start_time']), row['bee_id'], next_row['track_start_time'] - row['track_end_time'])\n",
    "                print(next_row['video_start_time'] - row['video_start_time'])\n",
    "                print(timestamp_to_string(row['track_start_time']),timestamp_to_string(row['track_end_time']),timestamp_to_string(row['video_start_time']),row['video_end_time'])\n",
    "                print(row['timestamps'])\n",
    "                print(row.name)\n",
    "                print(timestamp_to_string(next_row['track_start_time']))\n",
    "                print('----------------------------------')\n",
    "           \n",
    "            # update the timestamps of nextrow\n",
    "            t = next_row['video_start_time'] - row['video_start_time']\n",
    "            timestamps = list( np.array(next_row['timestamps']) + t)\n",
    "\n",
    "            # merge xpos, ypos, timestamps lists\n",
    "            tracks_ml.at[row.name,'xpos'] = row['xpos']+next_row['xpos']\n",
    "            tracks_ml.at[row.name,'ypos'] = row['ypos']+next_row['ypos']\n",
    "            tracks_ml.at[row.name,'timestamps'] = row['timestamps']+timestamps\n",
    "\n",
    "            # update end_time\n",
    "            tracks_ml.at[row.name,'track_end_time'] = next_row['track_end_time']\n",
    "            tracks_ml.at[row.name,'video_end_time'] = next_row['video_end_time']\n",
    "\n",
    "            # delete the merged row (next_row)\n",
    "            tracks_ml = tracks_ml.drop(next_row.name).copy()\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "        \n",
    "        if index == len(tracks_ml) - 1:\n",
    "            break\n",
    "            \n",
    "    # convert back to string\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: timestamp_to_string(x))\n",
    "            \n",
    "    return tracks_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:30.331249Z",
     "start_time": "2019-03-19T10:45:29.664321Z"
    }
   },
   "outputs": [],
   "source": [
    "tracks_ml = gather_tracks(tracks)\n",
    "tracks_ml = merge_tracks(tracks_ml)\n",
    "tracks_ml = tracks_ml.sort_values(['video_start_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:33.594997Z",
     "start_time": "2019-03-19T10:45:33.579105Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the test data csv (the manually labeled data)\n",
    "test_data = loadGTD(\"bees_test.csv\")\n",
    "test_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:34.176553Z",
     "start_time": "2019-03-19T10:45:34.171879Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data_len = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:34.732066Z",
     "start_time": "2019-03-19T10:45:34.699794Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the total number of tracks where the start_video and bee_id is correct\n",
    "correct_in_video_start = len(pd.merge(tracks_ml,test_data[['bee_id','in_direction','video_start_time']], on=['bee_id','video_start_time'], how='inner'))\n",
    "\n",
    "# get the total number of tracks where the end video and bee_id is correct\n",
    "correct_in_video_end = len(pd.merge(tracks_ml,test_data[['bee_id','out_direction','video_end_time']], on=['bee_id','video_end_time'], how='inner'))\n",
    "\n",
    "# get all tracks where bee_id, video_start_time and video_end_time are the same\n",
    "total_correct = len(pd.merge(tracks_ml,test_data[['bee_id','in_direction','out_direction','video_end_time','video_start_time']], how='inner',on=['bee_id','video_start_time','video_end_time']))\n",
    "\n",
    "print('bee_id and video_start_time correct: %.2f (%d/%d)' % (correct_in_video_start/test_data_len,correct_in_video_start,test_data_len))\n",
    "print('bee_id and video_end_time correct: %.2f (%d/%d)' % (correct_in_video_end/test_data_len,correct_in_video_end,test_data_len))\n",
    "print('bee_ids and both videos correct: %.2f (%d/%d)' % (total_correct/test_data_len,total_correct,test_data_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T13:36:37.259527Z",
     "start_time": "2019-03-18T13:36:37.163021Z"
    }
   },
   "source": [
    "# Add labels to tracks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:52.948793Z",
     "start_time": "2019-03-19T10:45:52.938825Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all tracks where bee_id, video_start_time and video_end_time are the same\n",
    "tracks_ml = pd.merge(tracks_ml,test_data,how='inner',on=['bee_id','video_start_time','video_end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:45:55.566508Z",
     "start_time": "2019-03-19T10:45:55.524896Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(tracks_ml))\n",
    "tracks_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmus 1: Baseline - Areas as Decider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T10:21:41.084108Z",
     "start_time": "2019-03-18T10:21:41.073270Z"
    },
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\"\"\" Vorgehen:\n",
    "Für jedes Bienen-Track Paar\n",
    "- Prüfe, wo sich die Biene beim ersten erkannten Erscheinen aufhält\n",
    "    - Ordne die Position in left, right oder middle ein\n",
    "- Prüfe, wo sich die Biene beim letzten erkannten Erscheinen aufhält\n",
    "    - Ordne die Position in left, right oder middle ein\n",
    "Wenn sich Biene in der Mitte befindet, starte neue Routine, die links oder rechts zuordnet\n",
    "Daraus kann nun abgeleitet werden, wo die Biene reingekommen ist und wo sie rausgegangen ist.\n",
    "\"\"\"\n",
    "# param: tracks_ml, s. oben\n",
    "def baseline_alg_classify_bee(tracks_ml):\n",
    "    def get_direction(xpos, zpos):\n",
    "        #Helpers\n",
    "        def is_left(xpos):\n",
    "            return xpos <= config[\"left_leaving_area\"]*config[\"px_x_resolution_vid\"]\n",
    "        def is_right(xpos):\n",
    "            return xpos >= config[\"px_x_resolution_vid\"] - config[\"right_leaving_area\"]*config[\"px_x_resolution_vid\"]\n",
    "        # Routine, wenn Biene in der Mitte\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        def middle_classifier(zRotation):\n",
    "            if abs(zRotation) > pi/2:\n",
    "                return \"left\"\n",
    "            else:\n",
    "                return \"right\"\n",
    "            \n",
    "        # Eintrittsseite festlegen\n",
    "        if is_left(xpos):\n",
    "            return \"left\"\n",
    "        elif is_right(xpos):\n",
    "            return \"right\"\n",
    "        else:\n",
    "            return middle_classifier(zpos)\n",
    "        \n",
    "    pred_in_direction = len(tracks_ml) * [None]\n",
    "    pred_out_direction = len(tracks_ml) * [None]\n",
    "    i = 0\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        pred_in_direction[i] = get_direction(row['xpos'][0], row['zrotation'][0])\n",
    "        pred_out_direction[i]= get_direction(row['xpos'][-1], row['zrotation'][-1])\n",
    "        i += 1\n",
    "    \n",
    "    #remove those with no \n",
    "    return pd.DataFrame({\"pred_in_direction\":pred_in_direction, \"pred_out_direction\":pred_out_direction})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T10:21:43.343660Z",
     "start_time": "2019-03-18T10:21:43.323873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ausgeben\n",
    "results = baseline_alg_classify_bee(tracks_ml)\n",
    "# write ergebnis to csv\n",
    "results\n",
    "#results.to_csv('MachineLearnedData.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmus 2: Baseline - zpos as decider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T16:05:43.933709Z",
     "start_time": "2019-03-15T16:05:43.923884Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\"\"\" Vorgehen:\n",
    "Unterschied: Hier wird nicht auf die Position abgestellt, sondern ausschließlich auf die\n",
    "Richtung, in die die Biene schaut\n",
    "\"\"\"\n",
    "# param: tracks_ml, s. oben\n",
    "def baseline_2_alg_classify_bee(tracks_ml):\n",
    "    def get_in_direction(zpos):\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        if abs(zpos) > abs(pi/2):\n",
    "            return \"right\"\n",
    "        else:\n",
    "            return \"left\"\n",
    "    def get_out_direction(zpos):\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        if abs(zpos) > abs(pi/2):\n",
    "            return \"left\"\n",
    "        else:\n",
    "            return \"right\"\n",
    "        \n",
    "    pred_in_direction = len(tracks_ml) * [None]\n",
    "    pred_out_direction = len(tracks_ml) * [None]\n",
    "    i = 0\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        pred_in_direction[i] = get_in_direction(row['zrotation'][0])\n",
    "        pred_out_direction[i]= get_out_direction(row['zrotation'][-1])\n",
    "        i += 1\n",
    "    \n",
    "    return pd.DataFrame({\"pred_in_direction\":pred_in_direction, \"pred_out_direction\":pred_out_direction})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T16:05:44.601495Z",
     "start_time": "2019-03-15T16:05:44.583396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ausgeben\n",
    "results = baseline_2_alg_classify_bee(tracks_ml)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmus 3: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereiten Lernvideos, Prüfvideos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Tracks horizontal invertieren; Tracks vertikal invertieren (Funktioniert, abhängig von der Wichtigkeit der Zeit als Feature, vielleicht nicht!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T10:21:51.264771Z",
     "start_time": "2019-03-18T10:21:51.258438Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:47:11.690758Z",
     "start_time": "2019-03-19T10:47:11.648734Z"
    },
    "code_folding": [
     4,
     9,
     12,
     18,
     33,
     45,
     70,
     86,
     102,
     119
    ]
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from math import pi\n",
    "import random\n",
    "\n",
    "def inc_year(video_time_string):\n",
    "    video_string_parts = video_time_string.split('-')\n",
    "    video_string_parts[0] = str(int(video_string_parts[0]) + 1)\n",
    "    return '-'.join(video_string_parts)\n",
    "\n",
    "def opposite(direction):\n",
    "            if direction == 'left':\n",
    "                return 'right'\n",
    "            else:\n",
    "                return 'left'\n",
    "\n",
    "\"\"\"\n",
    "invertiert einen Track auf der vertikalen Achse. Bei outer ist die Achse die Achse des Videos.\n",
    "\"\"\"\n",
    "def invertVertical_outer(tracks_ml):\n",
    "    inverted_tracks = tracks_ml.copy(deep = True)\n",
    "    center = config['px_y_resolution_vid']/2 #default\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        inverted_tracks.at[index, 'ypos'] = [abs(y - center) for y in row['ypos']]\n",
    "        inverted_tracks.at[index, 'zrotation'] = [z * -1 for z in row['zrotation']]\n",
    "        inverted_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        inverted_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        inverted_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        inverted_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "    return inverted_tracks\n",
    "\n",
    "\"\"\"\n",
    "invertiert einen Track auf der vertikalen Achse. Bei inner ist die Achse die Achse des Trackbereichs.\n",
    "\"\"\"\n",
    "def invertVertical_inner(tracks_ml):\n",
    "    inverted_tracks = tracks_ml.copy(deep = True)\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        center = max(row['ypos']) - (min(row['ypos'])/2)\n",
    "        inverted_tracks.at[index, 'ypos'] = [abs(y - center) for y in row['ypos']]\n",
    "        inverted_tracks.at[index, 'zrotation'] = [z * -1 for z in row['zrotation']]\n",
    "        inverted_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        inverted_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        inverted_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        inverted_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "    return inverted_tracks\n",
    "\n",
    "def invertHorizontal(tracks_ml):\n",
    "    inverted_tracks = tracks_ml.copy(deep = True)\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        center = max(row['xpos']) - (min(row['xpos'])/2)\n",
    "        inverted_tracks.at[index, 'xpos'] = [abs(x - center) for x in row['xpos']]\n",
    "        \n",
    "        def zrotation_shifter(z):\n",
    "            cmp = pi/2\n",
    "            if z >= cmp:\n",
    "                return z - cmp\n",
    "            else:\n",
    "                return z + cmp\n",
    "\n",
    "        inverted_tracks.at[index, 'zrotation'] = [zrotation_shifter(z) for z in row['zrotation']]\n",
    "        inverted_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        inverted_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        inverted_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        inverted_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "        inverted_tracks.at[index, 'in_direction'] = opposite(row['in_direction'])\n",
    "        inverted_tracks.at[index, 'out_direction'] = opposite(row['out_direction'])\n",
    "    return inverted_tracks\n",
    "\n",
    "\"\"\"\n",
    "randomisiert y Koordinaten\n",
    "\"\"\"\n",
    "def randomize_y_coordinates(tracks_ml):\n",
    "    threshold_y = 0.02 # Looking at the original videos, 2% equate to about 7px on the y-axis\n",
    "    inverted_tracks = tracks_ml.copy(deep = True)\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        lower_bound = -(config['px_y_resolution_vid'] * threshold_y)\n",
    "        upper_bound = config['px_y_resolution_vid'] * threshold_y\n",
    "        inverted_tracks.at[index, 'ypos'] = [y + random.uniform(lower_bound, upper_bound) for y in row['ypos']]\n",
    "        inverted_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        inverted_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        inverted_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        inverted_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "    return inverted_tracks\n",
    "    \n",
    "\"\"\"\n",
    "randomisiert x Koordinaten\n",
    "\"\"\"\n",
    "def randomize_x_coordinates(tracks_ml):\n",
    "    threshold_x = 0.01 # equates to 19 px\n",
    "    inverted_tracks = tracks_ml.copy(deep = True)\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        lower_bound = - config['px_x_resolution_vid'] * threshold_x\n",
    "        upper_bound = config['px_x_resolution_vid'] * threshold_x\n",
    "        inverted_tracks.at[index, 'xpos'] = [x + random.uniform(lower_bound, upper_bound) for x in row['xpos']]\n",
    "        inverted_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        inverted_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        inverted_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        inverted_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "    return inverted_tracks\n",
    "\n",
    "#\n",
    "# Biene geht andersherum -> koordinaten vertauschen, labels vertauschen\n",
    "#\n",
    "def reverse_path(tracks_ml):\n",
    "    reversed_tracks = tracks_ml.copy(deep = True)\n",
    "    for index, row in tracks_ml.iterrows():\n",
    "        reversed_tracks.at[index, 'xpos'] = list(reversed(row['xpos']))\n",
    "        reversed_tracks.at[index, 'ypos'] = list(reversed(row['ypos']))\n",
    "        reversed_tracks.at[index, 'video_start_time'] = inc_year(row['video_start_time'])\n",
    "        reversed_tracks.at[index, 'track_start_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][0]\n",
    "        reversed_tracks.at[index, 'track_end_time'] = string_to_timestamp(row['video_start_time']) + row['timestamps'][-1]\n",
    "        reversed_tracks.at[index, 'video_end_time'] = inc_year(row['video_end_time'])\n",
    "        reversed_tracks.at[index, 'in_direction'] = opposite(row['in_direction'])\n",
    "        reversed_tracks.at[index, 'out_direction'] = opposite(row['out_direction'])\n",
    "    return reversed_tracks\n",
    "\n",
    "\"\"\"\n",
    "Using powerset on a list of functions to try all possibilities\n",
    "param: DataFrame containing the standardized columns\n",
    "return: new DataFrame, containing also the original\n",
    "\"\"\"\n",
    "def augment_data(tracks_ml):\n",
    "    # taken from https://docs.python.org/3/library/itertools.html#recipes\n",
    "    # and slightly modified\n",
    "    def powerset(iterable):\n",
    "        \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "        s = list(iterable)\n",
    "        return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "    functions = [invertVertical_outer, invertVertical_inner, invertHorizontal,\n",
    "                 randomize_y_coordinates, randomize_x_coordinates, reverse_path]\n",
    "    powerset = powerset(functions)\n",
    "    length_powerset = 2**(len(functions))\n",
    "    result = tracks_ml\n",
    "    for i in range(length_powerset):\n",
    "        newDataFrame = tracks_ml.copy(deep = True)\n",
    "        for f in powerset[i]:\n",
    "            newDataFrame = f(newDataFrame)\n",
    "        result = result.append(newDataFrame, ignore_index = True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T14:34:38.783888Z",
     "start_time": "2019-03-19T14:34:38.777520Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(x, min_x, max_x):\n",
    "    \"\"\" normalize x to 0..1 \"\"\"\n",
    "    return (x-min_x)/(max_x-min_x)\n",
    "\n",
    "def get_time_from_timestamp(timestamp):\n",
    "    \"\"\" return time of day in hours (float) (range 0-23.99) \"\"\"\n",
    "    time_struct = time.localtime(timestamp)\n",
    "    return time_struct.tm_hour+normalize(time_struct.tm_min, 0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T14:53:58.947544Z",
     "start_time": "2019-03-19T14:53:58.931989Z"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1-p2)**2))\n",
    "\n",
    "def get_speed(p1, p2, t1, t2):\n",
    "    d = euclidean_distance(p1, p2)\n",
    "    t = t2-t1\n",
    "    return d/t\n",
    "\n",
    "def get_start_speed(xpos, ypos, timestamps):\n",
    "    \n",
    "    assert(len(xpos) == len(ypos) == len(timestamps))\n",
    "    \n",
    "    if len(xpos) < 2:\n",
    "        return 0\n",
    "    p1 = np.array([xpos[0], ypos[0]])\n",
    "    p2 = np.array([xpos[1], ypos[1]])\n",
    "    \n",
    "    return get_speed(p1,p2,timestamps[0],timestamps[1])\n",
    "\n",
    "def get_end_speed(xpos, ypos, timestamps):\n",
    "    \n",
    "    assert(len(xpos) == len(ypos) == len(timestamps))\n",
    "    \n",
    "    if len(xpos) < 2:\n",
    "        return 0\n",
    "    \n",
    "    p1 = np.array([xpos[-2], ypos[-2]])\n",
    "    p2 = np.array([xpos[-1], ypos[-1]])\n",
    "    return get_speed(p1,p2,timestamps[-2],timestamps[-1])\n",
    "\n",
    "def get_avg_speed(xpos, ypos, timestamps):\n",
    "    \"\"\" returns a list of speeds between the points (x_pos, y_pos) \"\"\"\n",
    "    if len(xpos) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    s = []\n",
    "    for i in range(len(xpos)-1):\n",
    "        p1 = np.array([xpos[i], ypos[i]])\n",
    "        p2 = np.array([xpos[i+1], ypos[i+1]])\n",
    "        \n",
    "        speed = get_speed(p1, p2, timestamps[i], timestamps[i+1])\n",
    "        s.append(speed)\n",
    "        \n",
    "    s = np.array(s)\n",
    "    return np.mean(s)\n",
    "\n",
    "def get_track_distance(xpos, ypos):\n",
    "    points = np.array([xpos, ypos]).T\n",
    "    \n",
    "    dists = []\n",
    "    for i in range(points.shape[0]-1):\n",
    "        d = euclidean_distance(points[i],points[i+1])\n",
    "        dists.append(d)\n",
    "    return np.sum(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T14:53:59.463662Z",
     "start_time": "2019-03-19T14:53:59.443637Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as sklearn_normalize\n",
    "\n",
    "def get_features(X, include=None, exclude=None, normalize=False):\n",
    "    \"\"\" \n",
    "    if include != None then only include features in include array, ignore exclude. default: include all\n",
    "    if exclude != None then inlcude all except exclude array. default: exclude None\n",
    "    \n",
    "    if normalize = True, normalizes all features to range [0,1]\n",
    "    \n",
    "    'duration',\n",
    "    'day_time',\n",
    "    'num_detections',\n",
    "    'start_xpos',\n",
    "    'end_xpos',\n",
    "    'avg_xpos',\n",
    "    'start_ypos',\n",
    "    'end_ypos',\n",
    "    'start_z_rotation',\n",
    "    'end_z_rotation',\n",
    "    'avg_zrotation',\n",
    "    'start_speed',\n",
    "    'end_speed',\n",
    "    'avg_speed',\n",
    "    'track_length'\n",
    "    \"\"\"\n",
    "    \n",
    "    # duration, start_pos, end_pos, start_z_rotation, end_z_rotation, time_of_day, num_points_in_track, avg_speed_of_bee, start_speed, end_speed\n",
    "    tracks = X.copy()\n",
    "    tracks_features = X.copy()\n",
    "    \n",
    "    tracks_features['duration'] = tracks['track_end_time'] - tracks['track_start_time']\n",
    "    tracks_features['start_xpos'] = tracks['xpos'].apply(lambda x: x[0])\n",
    "    tracks_features['end_xpos'] = tracks['xpos'].apply(lambda x: x[-1])\n",
    "    tracks_features['avg_xpos'] = tracks['xpos'].apply(lambda x: np.mean(x))\n",
    "    tracks_features['start_ypos'] = tracks['ypos'].apply(lambda x: x[0]) # do we need?\n",
    "    tracks_features['end_ypos'] = tracks['ypos'].apply(lambda x: x[-1]) # do we need?\n",
    "\n",
    "\n",
    "    tracks_features['normalized_zrotation'] =  tracks['zrotation'].apply(lambda x: np.array(x) + math.pi) # we normalized because pi -pi\n",
    "    tracks_features['start_z_rotation'] = tracks_features['normalized_zrotation'].apply(lambda x: x[0])\n",
    "    tracks_features['end_z_rotation'] = tracks_features['normalized_zrotation'].apply(lambda x: x[-1])\n",
    "    tracks_features['avg_zrotation'] =  tracks_features['normalized_zrotation'].apply(lambda x: np.mean(x))\n",
    "\n",
    "    tracks_features['day_time'] = (tracks['track_start_time'] + tracks['track_end_time']) / 2\n",
    "    tracks_features['day_time'] = tracks_features['day_time'].apply(lambda x: get_time_from_timestamp(x))\n",
    "\n",
    "    tracks_features['num_detections'] = tracks['xpos'].apply(lambda x: len(x))\n",
    "\n",
    "    # speed\n",
    "    tracks_features['start_speed'] = tracks[['xpos','ypos','timestamps']].apply(lambda x: get_start_speed(x[0],x[1],x[2]), axis=1)\n",
    "    tracks_features['end_speed'] = tracks[['xpos','ypos','timestamps']].apply(lambda x: get_end_speed(x[0],x[1],x[2]), axis=1)\n",
    "    tracks_features['avg_speed'] = tracks[['xpos','ypos','timestamps']].apply(lambda x: get_avg_speed(x[0],x[1],x[2]), axis=1)\n",
    "\n",
    "    # total distance traveled\n",
    "    tracks_features['track_length'] = tracks[['xpos','ypos']].apply(lambda x: get_track_distance(x[0],x[1]), axis=1)\n",
    "    \n",
    "    #must_include = ['bee_id','video_start_time','video_end_time','track_start_time','track_end_time','in_direction','out_direction']\n",
    "    all_features = ['duration','day_time','num_detections','start_xpos','end_xpos','avg_xpos','start_ypos','end_ypos','start_z_rotation','end_z_rotation','avg_zrotation','start_speed','end_speed','avg_speed', 'track_length']\n",
    "    if include is None and exclude is None:\n",
    "        columns = all_features\n",
    "    if include is not None:\n",
    "        columns = include\n",
    "    if exclude is not None:\n",
    "        columns = [x for x in all_features if x not in exclude]\n",
    "        \n",
    "    tracks_features = tracks_features[columns]\n",
    "    \n",
    "    if normalize is True:\n",
    "        \n",
    "        tracks_features = pd.DataFrame(sklearn_normalize(tracks_features, axis=0), columns=tracks_features.columns)\n",
    "        \n",
    "    return tracks_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Lernen Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:00:24.273879Z",
     "start_time": "2019-03-19T15:00:24.263303Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from math import pi\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# zeug von tschopo\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def train_log_regression_adv(data):\n",
    "    train_x, test_x = train_test_split(data, test_size = 0.5, random_state=random.randint(0,100))\n",
    "    # increase training set\n",
    "    train_x = augment_data(train_x)\n",
    "    train_y = train_x['in_direction']\n",
    "    train_x = get_features(train_x, normalize=False)\n",
    "    \n",
    "    # zeug von tschopo\n",
    "    selector = SelectKBest(mutual_info_classif, k=5)\n",
    "    selector.fit(train_x, train_y)\n",
    "    # Get columns to keep\n",
    "    cols = selector.get_support(indices=True)\n",
    "    # Create new dataframe with only desired columns, or overwrite existing\n",
    "    #X_new = X[cols]\n",
    "    #X_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n",
    "    #X_new = SelectKBest(mutual_info_classif, k=5).fit_transform(X, y)\n",
    "    col_names = train_x.columns[cols]\n",
    "    train_x = train_x#[col_names]\n",
    "    \n",
    "    # train on training set\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    # create test set\n",
    "    test_y = test_x['in_direction']\n",
    "    test_x = get_features(test_x)\n",
    "    test_x = test_x#[col_names]\n",
    "    \n",
    "    pred_y = clf.predict(test_x)\n",
    "    # calculate accuracy\n",
    "    #print(\"Genauigkeit: %f\" % accuracy_score(test_y, clf.predict(test_x)))\n",
    "    return float(accuracy_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:01:29.403388Z",
     "start_time": "2019-03-19T15:00:24.662888Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# taken from: https://stackoverflow.com/questions/783897/truncating-floats-in-python\n",
    "def truncate(f, n):\n",
    "    '''Truncates/pads a float f to n decimal places (after .) without rounding'''\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for i in range(1,11,1):\n",
    "    y.append(train_log_regression_adv(tracks_ml)*100)\n",
    "    #ohne random_state gab es teilw. gleiche Ergebnisse auf dem getesteten Macbook\n",
    "    \n",
    "x = range(1,len(y)+1,1)\n",
    "plt.plot(x, y,'bo')\n",
    "avg = sum(y)/float(len(y))\n",
    "plt.axhline(y=avg, color='r', linestyle='-')\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.yticks(np.arange((min(y)//10)*10-10, 110, 10.0))\n",
    "plt.xticks([])\n",
    "plt.annotate('avg. = '+ str(truncate(avg,1)), xy=(2/3*len(x),avg+2))\n",
    "plt.savefig('accuracy_basic_features.jpg', dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T13:37:56.751782Z",
     "start_time": "2019-03-19T13:37:56.745928Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(range(1,11,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Alg: Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:13:32.120403Z",
     "start_time": "2019-03-19T11:13:31.984635Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = tracks_ml.copy(deep=True)\n",
    "labels = data['in_direction']\n",
    "data = data.drop(columns=['out_direction', 'in_direction'])\n",
    "train_x, test_x = train_test_split(data, random_state = 42)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:04:51.785622Z",
     "start_time": "2019-03-19T11:04:51.778684Z"
    }
   },
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lernen Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Alg Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichsfunktion Algorithmus mit Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:01:46.441371Z",
     "start_time": "2019-03-15T11:01:46.361187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\" \n",
    "Read in two csv files and store in DataFrames. Merge DataFrames to get get correctnes of \n",
    "total-, in- and out_direction of bees regarding their in_videos and out_videos  \n",
    "\"\"\"\n",
    "\n",
    "# delete existing result.csv file to prevent errors\n",
    "if (os.path.isfile('./result.csv')):\n",
    "    os.remove(\"result.csv\")\n",
    "\n",
    "# store data in dataFrame\n",
    "gtd = loadGTD(\"bees_test.csv\")\n",
    "mld = pd.read_csv('MaschineLearnedData.csv')\n",
    "\n",
    "# delete dublications\n",
    "#print('Delete dublication from input.')\n",
    "mld = mld.drop_duplicates(['bee_id', 'time_in', 'time_out'], keep='first')\n",
    "gtd = gtd.drop_duplicates(['bee_id', 'time_in', 'time_out'], keep='first')\n",
    "\n",
    "# change DT from float to int to have the same DT like gtd\n",
    "mld.bee_id = mld.bee_id.astype('int32')\n",
    "\n",
    "# merge to get 100% correct matches\n",
    "mergeTotalCorrect = pd.merge(\n",
    "    gtd,\n",
    "    mld,\n",
    "    on=['bee_id', 'time_in', 'time_out', 'in_direction', 'out_direction'],\n",
    "    how='inner')\n",
    "#merge to get in_direction correct\n",
    "mergeInDirection = pd.merge(\n",
    "    gtd, mld, on=['bee_id', 'time_in', 'in_direction'], how='inner')\n",
    "#merge to get out_direction correct\n",
    "mergeOutDirection = pd.merge(\n",
    "    gtd, mld, on=['bee_id', 'time_out', 'out_direction'], how='inner')\n",
    "# merge to get comparable bees to get all found bees\n",
    "mergeableBees = pd.merge(\n",
    "    gtd, mld, on=['bee_id', 'time_in', 'time_out'], how='inner')\n",
    "\n",
    "# iterate mergeableBees for visualization\n",
    "result = pd.DataFrame()\n",
    "result['bee_id'] = mergeableBees['bee_id']\n",
    "result['in_equals'] = mergeableBees['in_direction_x'] == mergeableBees[\n",
    "    'in_direction_y']\n",
    "result['out_equals'] = mergeableBees['out_direction_x'] == mergeableBees[\n",
    "    'out_direction_y']\n",
    "\n",
    "# store to result.csv\n",
    "result.to_csv('result.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# calculate statitics\n",
    "inCorrect = round(result['in_equals'].sum() / len(result) * 100)\n",
    "outCorrect = round(result['out_equals'].sum() / len(result) * 100)\n",
    "allCorrect = round(\n",
    "    (result['in_equals'] == result['out_equals']).sum() / len(result) * 100)\n",
    "\n",
    "# print statistics\n",
    "try:\n",
    "    print(\n",
    "        len(gtd), 'entries in GroundTruthData.', len(mergeableBees),\n",
    "        'entries found to compare in MaschineLearnedData.')\n",
    "    print(allCorrect, \"% total correctness\")\n",
    "    print(inCorrect, \"% Ingoing correctness\")\n",
    "    print(outCorrect, \"% Outgoing correctness\")\n",
    "    print(\n",
    "        round(((allCorrect + inCorrect + outCorrect) / 3), 2),\n",
    "        \"% Average correctness\")\n",
    "except ZeroDivisionError:\n",
    "    print(\"Nothing to compare!\")\n",
    "\n",
    "# end compare algorithm\n",
    "print('\\nPlease check result.csv file or hit \\'result\\' for more details.')\n",
    "print(\n",
    "    'Check also DataFrames for deeper information: \\'mergeTotalCorrect\\',\\'mergeInCorrect\\',\\'mergeOutCorrect\\',\\'mergeableBees\\' '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:42:26.252092Z",
     "start_time": "2019-03-15T10:42:26.220981Z"
    }
   },
   "outputs": [],
   "source": [
    "mergeInDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T17:57:13.528244Z",
     "start_time": "2019-03-14T17:57:13.494603Z"
    }
   },
   "outputs": [],
   "source": [
    "mergeInDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T17:58:28.950948Z",
     "start_time": "2019-03-14T17:58:28.934916Z"
    }
   },
   "outputs": [],
   "source": [
    "mergeOutDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
