{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:06:35.968795Z",
     "start_time": "2019-03-15T11:06:35.961637Z"
    }
   },
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=theano\n",
    "%env THEANO_FLAGS=floatX=float32,device=cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:44:37.645425Z",
     "start_time": "2019-03-15T14:44:32.892042Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport bb_behavior\n",
    "%aimport bb_behavior.plot\n",
    "%aimport bb_behavior.tracking\n",
    "%aimport bb_behavior.tracking.pipeline\n",
    "\n",
    "import bb_behavior\n",
    "import bb_behavior.plot\n",
    "import bb_behavior.tracking\n",
    "import bb_behavior.tracking.pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:44:37.678294Z",
     "start_time": "2019-03-15T14:44:37.652293Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook # progress bar\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from bb_tracking.data.constants import DETKEY\n",
    "#from bb_tracking.tracking import score_id_sim_v\n",
    "from bb_tracking.tracking import distance_orientations_v, distance_positions_v\n",
    "\n",
    "from bb_behavior.tracking.pipeline import detect_markers_in_video\n",
    "from bb_behavior.tracking.pipeline import track_detections_dataframe\n",
    "from bb_behavior.tracking.pipeline import display_tracking_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:07:29.276297Z",
     "start_time": "2019-03-15T11:07:09.400050Z"
    }
   },
   "outputs": [],
   "source": [
    "from bb_behavior.tracking.pipeline import get_default_pipeline\n",
    "default_pipeline = None\n",
    "default_pipeline = get_default_pipeline(localizer_threshold=\"0.50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:44:41.898141Z",
     "start_time": "2019-03-15T14:44:41.890240Z"
    },
    "code_folding": [
     7
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hilfsfunktionen\n",
    "def filename_to_datestring(filname):\n",
    "    \"\"\"\n",
    "    filename can be path\n",
    "    \"\"\"\n",
    "    return os.path.split(filname)[-1].split('.')[0].split('_')[1]\n",
    "\n",
    "def datestring_to_filename(datestring, prefix=\"e00_\"):\n",
    "    return config[\"videos_dir\"] + prefix + datestring + \".h264\"\n",
    "\n",
    "def string_to_timestamp(datestring):\n",
    "    \"\"\" \n",
    "    params\n",
    "        string: format 2018-08-19-01-08-13\n",
    "    output\n",
    "        unix timestamp (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    return time.mktime(time.strptime(datestring, \"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "def timestamp_to_string(timestamp):\n",
    "    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:44:42.193994Z",
     "start_time": "2019-03-15T14:44:42.184308Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all videos between timestamp_in and timestamp_out\n",
    "def get_videos_between(timestamp_in, timestamp_out, all_paths):\n",
    "    \"\"\" returns all video between timestamp_in and timestamp out (inclusive) \"\"\"\n",
    "    \n",
    "    mask = (all_paths['video'] >= timestamp_in) & (all_paths['video'] <= timestamp_out)\n",
    "    return list(all_paths[mask]['video'])\n",
    "\n",
    "def loadGTD(path):\n",
    "    # read in the test data csv\n",
    "    test_data = pd.read_csv(path)\n",
    "\n",
    "    # convert the full filenames to string timestamps, and sort by timestamp_in\n",
    "    test_data['video'] = test_data['video'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data['video_start_time'] = test_data['timestamp_in'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data['video_end_time'] = test_data['timestamp_out'].apply(lambda x: filename_to_datestring(x))\n",
    "    test_data.sort_values(['video_start_time'], inplace=True)\n",
    "\n",
    "    test_data.drop('timestamp_in', 1, inplace=True)\n",
    "    test_data.drop('timestamp_out', 1, inplace=True)\n",
    "    test_data.drop('video', 1, inplace=True)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:46:49.920438Z",
     "start_time": "2019-03-15T14:46:49.882532Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = loadGTD(\"bees_test.csv\")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T13:10:50.120825Z",
     "start_time": "2019-03-15T13:10:50.114077Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = dict(tag_pixel_diameter=50,\n",
    "              n_frames=None,\n",
    "              confidence_filter_detections=0.08,\n",
    "              confidence_filter_tracks=0.20,\n",
    "              coordinate_scale=1.0,\n",
    "              start_time=None,\n",
    "              fps=10.0,\n",
    "              cam_id=0,\n",
    "              left_leaving_area = 0.3, # Prozente vom Bildschirmrand, zB. bei 1000px und 0.15 -> 0-150px\n",
    "              right_leaving_area = 0.3,\n",
    "              px_x_resolution_vid = 1944,\n",
    "              videos_dir = \"../videos/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:33:28.429823Z",
     "start_time": "2019-03-14T13:33:28.414495Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_tracks(paths, save_to_csv = False):\n",
    "    # Hier passiert das eigentliche Tracken und speichern der Ergebnisse:\n",
    "    num_processed_videos = 0\n",
    "    video_data = dict()\n",
    "    frame_info = None\n",
    "    detections = None\n",
    "    bad_paths = []\n",
    "\n",
    "    for path in tqdm_notebook(paths):\n",
    "        start_time = config[\"start_time\"]\n",
    "        cam_id = config[\"cam_id\"]\n",
    "        try:\n",
    "            num_processed_videos += 1\n",
    "\n",
    "            frame_info, detections = detect_markers_in_video(path,\n",
    "                                                          decoder_pipeline=default_pipeline,#pipeline=pipelines(),\n",
    "                                                         tag_pixel_diameter=config[\"tag_pixel_diameter\"],\n",
    "                                                          n_frames=config[\"n_frames\"],\n",
    "                                                          fps=config[\"fps\"],\n",
    "                                                         progress=\"tqdm_notebook\"\n",
    "                                                )\n",
    "            # Sonst würden keine Tracks erkannt werden -> Fehlermeldung\n",
    "            if len(detections[detections['confidence']>=config[\"confidence_filter_detections\"]]) == 0:\n",
    "                continue\n",
    "            tracks = track_detections_dataframe(detections,\n",
    "                                                tracker=\"tracker.det_score_fun.frag_score_fun.dill\",\n",
    "                                                confidence_filter_detections=config[\"confidence_filter_detections\"],\n",
    "                                               confidence_filter_tracks=config[\"confidence_filter_tracks\"],\n",
    "                                                coordinate_scale=config[\"coordinate_scale\"],\n",
    "                                               )\n",
    "            date_string = filename_to_datestring(path)\n",
    "            tracks['video'] = date_string\n",
    "            video_data[path] = (frame_info, detections, tracks)\n",
    "        except ValueError as err: #tritt auf, wenn Video leer ist. In diesem Fall: überspringe video\n",
    "            try:\n",
    "                bad_paths.append(path)\n",
    "                # wir arbeiten später nochmal mit paths, daher müssen das leere löschen, weil sonst\n",
    "                # in video_data kein zugehöriger Value zu Key = file zu finden ist.\n",
    "            except KeyError as err:\n",
    "                continue\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            raise\n",
    "        # only first vid: break\n",
    "\n",
    "    for bad_path in bad_paths:\n",
    "        paths.remove(bad_path)\n",
    "\n",
    "    #video_data[\"file\"][0] --> frame-info, [1] --> detections, [2] --> tracks\n",
    "    for path in paths:\n",
    "        display_tracking_results(path, video_data[path][0], video_data[path][1], video_data[path][2])\n",
    "\n",
    "    tracks = [video_data[paths[x]][2] for x in range(len(paths))]\n",
    "    tracks = pd.concat(tracks,ignore_index=True)\n",
    "    tracks = tracks.drop(columns=[\"localizerSaliency\", \"beeID\", \"camID\", \"frameIdx\"])\n",
    "\n",
    "    if save_to_csv:\n",
    "        with open(\"tracks.csv\", \"w\") as f:\n",
    "            tracks.to_csv(f)\n",
    "\n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a: Create video path list from all videos in videos_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:33:39.751272Z",
     "start_time": "2019-03-14T13:33:39.745103Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Einen Iterable speichern, der alle Videos in einem Iterable zur Verfügung stellt\n",
    "# Diesen Iterable können wir dann in der nächsten Zelle mit tqdm schön durchlaufen\n",
    "base_directory = config[\"videos_dir\"]\n",
    "paths = [i for i in os.listdir(base_directory) if i.endswith(\".h264\")]\n",
    "for i in range(len(paths)):\n",
    "    paths[i] = base_directory + paths[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b: Create video path list from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:34:54.196867Z",
     "start_time": "2019-03-15T10:34:54.066665Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test data csv\n",
    "test_data = loadGTD(\"bees_test.csv\")\n",
    "\n",
    "all_videos = []\n",
    "\n",
    "all_paths = pd.DataFrame(glob.glob(os.path.join(config[\"videos_dir\"], '*.h264')), columns=['video'])\n",
    "all_paths['video'] = all_paths['video'].apply(lambda x: filename_to_datestring(x))\n",
    "all_paths.sort_values(['video'])\n",
    "\n",
    "# go through test_data and get all videos between timestamp_in and timestamp_out\n",
    "for index, row in test_data.iterrows():\n",
    "    all_videos += get_videos_between(row['video_start_time'],row['video_end_time'], all_paths)\n",
    "    \n",
    "all_videos = sorted(list(set(all_videos)))\n",
    "paths = [datestring_to_filename(x) for x in all_videos]\n",
    "del all_paths\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:35:09.468983Z",
     "start_time": "2019-03-15T10:35:09.453016Z"
    }
   },
   "source": [
    "tracks = detect_tracks(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwischenschritt: Merge all close Tracks of one bee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:15:33.270108Z",
     "start_time": "2019-03-15T14:15:33.236959Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS IF TRACKS WHERE ALREADY CALCULATED AND SAVED TO AN .csv\n",
    "# CSV EINLESEN UND SETZEN\n",
    "tracks = pd.read_csv(\"all_tracks.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:15:33.954423Z",
     "start_time": "2019-03-15T14:15:33.930964Z"
    },
    "code_folding": [
     0,
     39
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_tracks(tracks):\n",
    "    \"\"\"\n",
    "    transform the tracks df to : bee_id, [list of positions (x,y)], [list of timestamps], timestamp_of video\n",
    "    \"\"\"\n",
    "\n",
    "    #  transform tracks df to : bee_id, [list of positions (x,y)], [list of timestamps], timestamp_of video\n",
    "    grouped = tracks.groupby(['bee_id','track_id','video'],  as_index=False)['xpos','ypos','timestamp','zrotation']\n",
    "\n",
    "    tracks_ml = grouped.aggregate(lambda x: list(x))\n",
    "\n",
    "    # we don't need track_id anymore\n",
    "    tracks_ml = tracks_ml.drop('track_id', 1)\n",
    "\n",
    "    # add a column: convert video name to timestamp\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video'].apply(lambda x: string_to_timestamp(x))\n",
    "    \n",
    "    # we don't need video anymore\n",
    "    tracks_ml = tracks_ml.drop('video', 1)\n",
    "\n",
    "    # because aggregated: now multiple timestamps per row: --> rename\n",
    "    tracks_ml = tracks_ml.rename(columns={'timestamp': 'timestamps'})\n",
    "\n",
    "    # calculate start time of track by adding timestamp of track (seconds since start of video)\n",
    "    # to timestamp of video (date)\n",
    "    tracks_ml['track_start_time'] = tracks_ml['video_start_time'] + tracks_ml['timestamps'].apply(lambda x: x[0])\n",
    "    tracks_ml['track_end_time'] = tracks_ml['video_start_time'] + tracks_ml['timestamps'].apply(lambda x: x[-1])\n",
    "    \n",
    "    # convert back to string\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: timestamp_to_string(x))\n",
    "    \n",
    "    all_paths = pd.DataFrame(glob.glob(os.path.join(config[\"videos_dir\"], '*.h264')), columns=['video'])\n",
    "    all_paths['video'] = all_paths['video'].apply(lambda x: filename_to_datestring(x))\n",
    "    all_paths.sort_values(['video'])\n",
    "    # get end video\n",
    "    tracks_ml['video_end_time'] =  tracks_ml['track_end_time'].apply(lambda x: timestamp_to_string(x))\n",
    "    tracks_ml['video_end_time'] =  tracks_ml[['video_start_time','video_end_time']].apply(lambda x: get_videos_between(x[0],x[1],all_paths)[-1], axis=1)\n",
    "\n",
    "    return tracks_ml\n",
    "\n",
    "def merge_tracks(tracks_ml, verbose = False):\n",
    "    \"\"\"\n",
    "    in: \n",
    "        tracks_ml: output from gather_tracks(tracks)\n",
    "    out:\n",
    "        same df as in, with merged rows\n",
    "        [bee_id:Float, xpos:[Float], ypos:[Float], zrotation:[Float], timestamps:[Float], video_start_time: String,\n",
    "        video_end_time: String, track_start_time:Float, track_end_time:Float]\n",
    "        \n",
    "    merge tracks of same bee where start and end timestamps are close together\n",
    "    assume there can not be overlapping tracks\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 1. sort: bee_id, start_time\n",
    "    tracks_ml = tracks_ml.sort_values(['bee_id', 'track_start_time'])\n",
    "\n",
    "    \n",
    "    # first convert to timestamp\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: string_to_timestamp(x))\n",
    "    \n",
    "    index = 0\n",
    "    while(True):\n",
    "        row = tracks_ml.iloc[index]\n",
    "        next_row = tracks_ml.iloc[index+1]\n",
    "\n",
    "        # if the tracks are from same bee and the start time of next row is closer then 10s -> merge the rows\n",
    "        # merge rows means, next_row is deleted\n",
    "        if (row['bee_id'] == next_row['bee_id']) and ((next_row['track_start_time'] - row['track_end_time']) < 10):\n",
    "            \n",
    "            if verbose:\n",
    "                print(timestamp_to_string(row['video_start_time']), row['bee_id'], next_row['track_start_time'] - row['track_end_time'])\n",
    "                print(next_row['video_start_time'] - row['video_start_time'])\n",
    "                print(timestamp_to_string(row['track_start_time']),timestamp_to_string(row['track_end_time']),timestamp_to_string(row['video_start_time']),row['video_end_time'])\n",
    "                print(row['timestamps'])\n",
    "                print(row.name)\n",
    "                print(timestamp_to_string(next_row['track_start_time']))\n",
    "                print('----------------------------------')\n",
    "           \n",
    "            # update the timestamps of nextrow\n",
    "            t = next_row['video_start_time'] - row['video_start_time']\n",
    "            timestamps = list( np.array(next_row['timestamps']) + t)\n",
    "\n",
    "            # merge xpos, ypos, timestamps lists\n",
    "            tracks_ml.at[row.name,'xpos'] = row['xpos']+next_row['xpos']\n",
    "            tracks_ml.at[row.name,'ypos'] = row['ypos']+next_row['ypos']\n",
    "            tracks_ml.at[row.name,'timestamps'] = row['timestamps']+timestamps\n",
    "\n",
    "            # update end_time\n",
    "            tracks_ml.at[row.name,'track_end_time'] = next_row['track_end_time']\n",
    "            tracks_ml.at[row.name,'video_end_time'] = next_row['video_end_time']\n",
    "\n",
    "            # delete the merged row (next_row)\n",
    "            tracks_ml = tracks_ml.drop(next_row.name).copy()\n",
    "\n",
    "        else:\n",
    "            index += 1\n",
    "        \n",
    "        if index == len(tracks_ml) - 1:\n",
    "            break\n",
    "            \n",
    "    # convert back to string\n",
    "    tracks_ml['video_start_time'] = tracks_ml['video_start_time'].apply(lambda x: timestamp_to_string(x))\n",
    "            \n",
    "    return tracks_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:15:35.322809Z",
     "start_time": "2019-03-15T14:15:34.696105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracks_ml = gather_tracks(tracks)\n",
    "tracks_ml = merge_tracks(tracks_ml)\n",
    "tracks_ml = tracks_ml.sort_values(['video_start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:15:35.507332Z",
     "start_time": "2019-03-15T14:15:35.325800Z"
    }
   },
   "outputs": [],
   "source": [
    "tracks_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test data csv\n",
    "test_data = loadGTD(\"bees_test.csv\")\n",
    "\n",
    "# get all tracks where bee_id, videio_start_time and video_end_time are the same\n",
    "tracks_ml = pd.merge(tracks_ml,test_data,how='inner',on=['bee_id','video_start_time','video_end_time'])\n",
    "\n",
    "# we only look at tracks where  in_direction and out_direction are different\n",
    "tracks_ml = tracks_ml[(tracks_ml['in_direction'] != tracks_ml['out_direction'])]\n",
    "\n",
    "# we assign movement based on in_ and out_directoin\n",
    "tracks_ml.loc[(tracks_ml['in_direction'] == 'left') & (tracks_ml['out_direction'] == 'right'),'movement'] = 1\n",
    "tracks_ml.loc[(tracks_ml['in_direction'] == 'right') & (tracks_ml['out_direction'] == 'left'),'movement'] = 0\n",
    "\n",
    "tracks_ml.drop('in_direction', 1, inplace=True)\n",
    "tracks_ml.drop('out_direction', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmus 1: Baseline - Areas as Decider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:37:39.246398Z",
     "start_time": "2019-03-15T10:37:39.232098Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\"\"\" Vorgehen:\n",
    "Für jedes Bienen-Track Paar\n",
    "- Prüfe, wo sich die Biene beim ersten erkannten Erscheinen aufhält\n",
    "    - Ordne die Position in left, right oder middle ein\n",
    "- Prüfe, wo sich die Biene beim letzten erkannten Erscheinen aufhält\n",
    "    - Ordne die Position in left, right oder middle ein\n",
    "Wenn sich Biene in der Mitte befindet, starte neue Routine, die links oder rechts zuordnet\n",
    "Daraus kann nun abgeleitet werden, wo die Biene reingekommen ist und wo sie rausgegangen ist.\n",
    "\"\"\"\n",
    "# param: tracks_ml, s. oben\n",
    "def baseline_alg_classify_bee(tracks_ml):\n",
    "    def get_direction(xpos, zpos):\n",
    "        #Helpers\n",
    "        def is_left(xpos):\n",
    "            return xpos <= config[\"left_leaving_area\"]*config[\"px_x_resolution_vid\"]\n",
    "        def is_right(xpos):\n",
    "            return xpos >= config[\"px_x_resolution_vid\"] - config[\"right_leaving_area\"]*config[\"px_x_resolution_vid\"]\n",
    "        # Routine, wenn Biene in der Mitte\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        def middle_classifier(zRotation):\n",
    "            if abs(zRotation) > pi/2:\n",
    "                return \"left\"\n",
    "            else:\n",
    "                return \"right\"\n",
    "            \n",
    "        # Eintrittsseite festlegen\n",
    "        if is_left(xpos):\n",
    "            return \"left\"\n",
    "        elif is_right(xpos):\n",
    "            return \"right\"\n",
    "        else:\n",
    "            return middle_classifier(zpos)\n",
    "    \n",
    "    # gets directions return movement as int\n",
    "    def movement_decider(in_dir, out_dir):\n",
    "        if in_dir == \"left\" and out_dir == \"right\":\n",
    "            return 1\n",
    "        elif in_dir == \"right\" and out_dir == \"left\":\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    \n",
    "    pred_movement = np.empty(len(tracks_ml))\n",
    "    int index = 0\n",
    "    for row in tracks_ml.itertuples(index=False):\n",
    "        pred_movement[index] = movement_decider(get_in_direction(row[4][0]), get_out_direction(row[4][-1])\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T10:37:43.118817Z",
     "start_time": "2019-03-15T10:37:43.096878Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ausgeben\n",
    "results = baseline_alg_classify_bee(tracks_ml)\n",
    "results = pd.DataFrame(data=results)\n",
    "# write ergebnis to csv\n",
    "results.to_csv('MachineLearnedData.csv', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmus 2: Baseline - zpos as decider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:39:44.728174Z",
     "start_time": "2019-03-14T13:39:44.716229Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\"\"\" Vorgehen:\n",
    "Unterschied: Hier wird nicht auf die Position abgestellt, sondern ausschließlich auf die\n",
    "Richtung, in die die Biene schaut\n",
    "\"\"\"\n",
    "#results = {\"bee_id\":[], \"time_in\":[], \"os_in\":[], \"time_out\":[], \"os_out\":[], \"in_direction\":[], \"out_direction\":[]}\n",
    "# param: tracks_ml, s. oben\n",
    "def baseline_2_alg_classify_bee(tracks_ml):\n",
    "    def get_in_direction(zpos):\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        if zpos > abs(pi/2):\n",
    "            return \"right\"\n",
    "        else:\n",
    "            return \"left\"\n",
    "    def get_out_direction(zpos):\n",
    "        # Betrachte zRotation, das ist die Richtung in die die Biene guckt (in Bogenmaß)\n",
    "        # gebe diese Richtung aus\n",
    "        if zpos > abs(pi/2):\n",
    "            return \"left\"\n",
    "        else:\n",
    "            return \"right\"\n",
    "    \n",
    "    # \"bee_id\":[int], \"time_in\":[String], \"os_in\":[Float], \"time_out\":[String],\n",
    "    # \"os_out\":[Float], \"in_direction\":[{\"left\", \"right\"}], \"out_direction\":[{\"left\", \"right\"}]\n",
    "    results = {\"bee_id\":[], \"time_in\":[], \"os_in\":[], \"time_out\":[], \"os_out\":[], \"in_direction\":[],\n",
    "               \"out_direction\":[]}\n",
    "        \n",
    "    for row in tracks_ml.itertuples(index=False):\n",
    "        results[\"bee_id\"].append(row[0])\n",
    "        results[\"time_in\"].append(row[5])\n",
    "        results[\"os_in\"].append(row[-3] - string_to_timestamp(row[5])) #track_start_time - video_start_time\n",
    "        results[\"time_out\"].append(row[-1])\n",
    "        results[\"os_out\"].append(row[-2] - string_to_timestamp(row[-1])) #track_end_time - video_end_time\n",
    "        results[\"in_direction\"].append(get_in_direction(row[4][0]))\n",
    "        results[\"out_direction\"].append(get_out_direction(row[4][-1]))\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:39:45.108992Z",
     "start_time": "2019-03-14T13:39:45.081669Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ausgeben\n",
    "results = baseline_2_alg_classify_bee(tracks_ml)\n",
    "results = pd.DataFrame(data=results)\n",
    "# write ergebnis to csv\n",
    "results.to_csv('MachineLearnedData.csv', encoding=\"utf-8\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmus 3: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereiten Lernvideos, Prüfvideos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Tracks um 180° drehen; Tracks horizontal invertieren; Tracks vertikal invertieren (Funktioniert, abhängig von der Wichtigkeit der Zeit als Feature, vielleicht nicht!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T12:41:18.043523Z",
     "start_time": "2019-03-15T12:41:18.038981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tracks um 180° drehen\n",
    "track_rotated = tracks_ml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Lernen Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Alg: Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lernen Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Alg Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichsfunktion Algorithmus mit Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T11:01:46.441371Z",
     "start_time": "2019-03-15T11:01:46.361187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy():\n",
    "    \"\"\" \n",
    "    Input: bees_test.csv, MachineLearnedData.csv\n",
    "    Output: \"result\" as DataFrame with an overview of compareable bees adn their correct/wrong correctness\n",
    "    \n",
    "    Read in two csv file and store values in DataFrame's. Merge DataFrame's to get exact correctness of \n",
    "    total-, in- and out_direction of bees regarding their time_in and time_out relation. \n",
    "    \"\"\"\n",
    "\n",
    "    # delete existing result.csv file to prevent errors\n",
    "    if (os.path.isfile('./result.csv')):\n",
    "        os.remove(\"result.csv\")\n",
    "\n",
    "    # store data in dataFrame\n",
    "    gtd = loadGTD('bees_test.csv')\n",
    "    mld = pd.read_csv('MachineLearnedData.csv')\n",
    "\n",
    "    # change column name to merge dataFrames\n",
    "    gtd.rename(columns = {'video_start_time':'time_in', 'video_end_time':'time_out'}, inplace=True)\n",
    "\n",
    "    # delete dublications\n",
    "    mld = mld.drop_duplicates(['bee_id', 'time_in', 'time_out'], keep='first')\n",
    "    gtd = gtd.drop_duplicates(['bee_id', 'time_in', 'time_out'], keep='first')\n",
    "\n",
    "    # change DT from float to int to have the same DT like gtd\n",
    "    mld.bee_id = mld.bee_id.astype('int32')\n",
    "\n",
    "    # merge to get 100% correct matches\n",
    "    mergeTotalCorrect = pd.merge(gtd, mld, on=['bee_id', 'time_in', 'time_out', 'in_direction', 'out_direction'], how='inner')\n",
    "    #merge to get in_direction correct\n",
    "    mergeInDirection = pd.merge(gtd, mld, on=['bee_id', 'time_in', 'in_direction'], how='inner')\n",
    "    #merge to get out_direction correct\n",
    "    mergeOutDirection = pd.merge(gtd, mld, on=['bee_id', 'time_out', 'out_direction'], how='inner')\n",
    "    # merge to get comparable bees to get all found bees\n",
    "    mergeableBees = pd.merge(gtd, mld, on=['bee_id', 'time_in', 'time_out'], how='inner')\n",
    "\n",
    "    # iterate mergeableBees for visualization\n",
    "    result = pd.DataFrame()\n",
    "    result['bee_id'] = mergeableBees['bee_id'] \n",
    "    result['in_equals'] = mergeableBees['in_direction_x'] == mergeableBees['in_direction_y']\n",
    "    result['out_equals'] = mergeableBees['out_direction_x'] == mergeableBees['out_direction_y']\n",
    "    #result['movement'] = result['in_equals'].bool() == True and result['out_equals'].bool() == False\n",
    "\n",
    "    # store to result.csv\n",
    "    result.to_csv('result.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    # calculate statitics\n",
    "    inCorrect = round((result['in_equals'].sum()/len(result) * 100), 2)\n",
    "    outCorrect = round((result['out_equals'].sum()/len(result) * 100), 2)\n",
    "    allCorrect = round(((result['in_equals'] == result['out_equals']).sum()/len(result) * 100), 2)\n",
    "    aveCorrect = round(((allCorrect + inCorrect + outCorrect) / 3), 2)\n",
    "\n",
    "    # print statistics\n",
    "    try:\n",
    "        print(len(gtd), 'entries in GroundTruthData.', len(mergeableBees), 'entries compareable in MachineLearnedData: ', round((len(mergeableBees)/len(gtd))*100, 2), '%' )\n",
    "        print(allCorrect, \"% total correctness.\")\n",
    "        print(inCorrect, \"% Ingoing correctness.\")\n",
    "        print(outCorrect, \"% Outgoing correctness.\")\n",
    "        print(aveCorrect, \"% Average correctness.\")\n",
    "        print(\"Accuracy of \",round((len(mergeableBees)/len(gtd))*100*(allCorrect/100),2),\"%.\")\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Nothing to compare!\")\n",
    "\n",
    "    # end compare algorithm\n",
    "    print('\\nCheck result.csv file or hit \\'result\\' for more details.')\n",
    "    print('Use DataFrames for deeper information: \\n\\'mergeTotalCorrect\\',\\'mergeInCorrect\\',\\'mergeOutCorrect\\',\\'mergeableBees\\' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
